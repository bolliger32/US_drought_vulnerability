# US Drought Vulnerability

## UC Berkeley DS421 / Statistics 259 Project
### Spring 2016

_**Research question:**_ What makes different communities more or less resilient to drought?

## Overview
This repository contains the files needed to reproduce our analysis of variation in vulnerability to drought across U.S. counties.  We conduct a two-stage analysis.  First, we find the correlation between drought level (as reported by the USDA Drought Monitor) and change in various indicators of welfare, including unemployment, infant mortality, and homelessness.  This correlation coefficient can be interpreted as the county's "vulnerability" to drought.  Second, we investigate which demographic characteristics predict vulnerability by regressing the coefficient from the first stage on a set of demographic variables such as race, age distribution, income, and more.
Please follow the insturctions to explore on the repository.

## Directions
1. Clone the repo: `git clone https://github.com/bolliger32/US_drought_vulnerability.git'
2. Install necessary software. You will need:

-  python
-  R
-  ArcGIS
-  ...

### Navigation
 - Data `make data` : Downloads the required data sets, cleans them, and merges them into the working data set. The total size of the file is ~XXXXX.
 - Report `make report` : Creates final_report.pdf under paper/ and cleans the paper/ directory

FROM HERE DOWN, this is from project_epsilon and should be used as a template for us to modify

 - Validate `make validate_data` : Validates the downloaded data 

 - Clean `make clean` : remove compiled python files

 - Test `make test` : Tests the functions in code/utils folder

 - Coverage `make coverage` : Creates a coverage report for the functions in code/utils/ folder

 - Verbose `make verbose` : Tests the functions in code/utils folder via nosetests option



 - Analysis for Subject 1 and 5 `make analysis-except-multi` : Executes all analysis (
 except for the multi comparison) and creates relevant  img files under fig/ folder
 	 - NOTICE : `make multi-comparison` will run about for 1 hour because it 
	 has to generate all the beta values for each single voxel for each subject 
	 over time-course.
 - To make all analyses `make analysis-except-multi` and then `make multi-comparison`

 - To conduct steps of analysis separately :
   - `make eda` 
   - `make linear`
   - `make logistic`
   - `make convolution-high`
   - `make convolution-normal`
   - `make t-test`
   - `make glm`
   - `make noise-pca`
   - `make multi-comparison`


## Contributors

- Daniel Blaustein-Rejito ([`linkname`](link to your github site))
- Ian Bolliger ([`linkname`](link to your github site))
- Hal Gordon ([`linkname`](link to your github site))
- Andrew Hultgren ([`linkname`](link to your github site))
- Yang Ju ([`linkname`](link to your github site))
- Kate Pennington ([`linkname`](link to your github site))
- Sara Stoudt ([`linkname`](link to your github site))
