## homelessness/weather data expo
## S.A.S.
## 2016-02-06


wd="~/Desktop/drought_project/data/homeless" ## replace with your own path name
setwd(wd)

require(xlsx)
require(maptools)
require(downloader)

## download from here:
##https://www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/

# coc = continuum of care
byCoC=read.xlsx("2007-2015-PIT-Counts-by-CoC.xlsx",1,stringsAsFactors=FALSE)
#byState=read.xlsx("2007-2015-PIT-Counts-by-State.xlsx",1)
#housing=read.xlsx("2007-2015-HIC-Counts-by-CoC.xlsx",1)
#housing=read.xlsx("tryThis.xlsx",1) # had to manually take out blank rows and columns
# copied and pasted non-blank entries in the spreadsheet


#names(housing)
#names(byState)
names(byCoC)
head(byCoC$CoC.Name)

## 2014 CoC shape files
## download all COC shapefiles here
## https://www.hudexchange.info/coc/gis-tools/

##download csv of state abbreviations from here
## http://statetable.com without territories, with DC and without military bases or minor possessions

state = read.csv("state_lookup_table.csv",1,stringsAsFactors=FALSE) 

##download, extract and read shapefile zip files 
#ST = state acronym

for (row in 1:length(state$abbreviation)) {
  ST = paste(state$abbreviation[row],'_2014','.zip',sep="")
  url = paste("https://www.hudexchange.info/resource/reportmanagement/published/CoC_GIS_State_Shapefile_",ST,sep="") 
  download(url,ST, mode="wb")
  unzip (ST, exdir = state$abbreviation[row])
  #Add command to remove zip file
}

##IN PROGRESS For each state, create a variable equal to each CoC Shapefile
for (row in 1:length(state$abbreviation)) {
  ST = state$abbreviation[row]
  for (file in length(list.files(ST))){
    for byCoC$CoC.Number[row] = readShapePoly()
  }
    

ca500=readShapePoly("CA_500.shp") 
wd="~/Desktop/ds421_brainstorm/California/CA_501" ## replace with your own path name
setwd(wd) 
ca501=readShapePoly("CA_501.shp")
attributes(ca500)
plot(ca500)
names(ca500)

ca500$ARD

test = rbind(ca500, ca501, makeUniqueIDs = TRUE)
plot(test)


## read each shape file in, combine them
## following code is really gross but it gets the job done

wd="~/Desktop/ds421_brainstorm/" ## set to where you keep the shape file data, end with slash

caFull=ca500
ard=caFull$ARD
pprn=caFull$PPRN
finalStatus=as.character(caFull$FPRN_STATU)
for(i in 1:9){
  setwd(paste(wd,"California/CA_50",i,sep=""))
  ca=readShapePoly(paste("CA_50",i,".shp",sep=""))
  caFull=rbind(caFull,ca,makeUniqueIDs=T)
  ard=c(ard,ca$ARD)
  pprn=c(pprn,ca$PPRN)
  finalStatus=c(finalStatus,as.character(ca$FPRN_STATU))
}
plot(caFull)
for(i in c(10:22,24:26)){
  setwd(paste(wd,"California/CA_5",i,sep=""))
  ca=readShapePoly(paste("CA_5",i,".shp",sep=""))
  caFull=rbind(caFull,ca,makeUniqueIDs=T)
  ard=c(ard,ca$ARD)
  pprn=c(pprn,ca$PPRN)
  finalStatus=c(finalStatus,as.character(ca$FPRN_STATU))}
plot(caFull)

for(i in c(1:4,6:9)){
  setwd(paste(wd,"California/CA_60",i,sep=""))
  ca=readShapePoly(paste("CA_60",i,".shp",sep=""))
  caFull=rbind(caFull,ca,makeUniqueIDs=T)
  ard=c(ard,ca$ARD)
  pprn=c(pprn,ca$PPRN)
  finalStatus=c(finalStatus,as.character(ca$FPRN_STATU))}

for(i in c(11:14)){
  setwd(paste(wd,"California/CA_6",i,sep=""))
  ca=readShapePoly(paste("CA_6",i,".shp",sep=""))
  caFull=rbind(caFull,ca,makeUniqueIDs=T)
  ard=c(ard,ca$ARD)
  pprn=c(pprn,ca$PPRN)
  finalStatus=c(finalStatus,as.character(ca$FPRN_STATU))
}
map("state","california")
plot(caFull,add=T,col="red")

hist(ard)
hist(pprn)
table(finalStatus)

## color code by different information in shape files
## see metadata folder for context
require(RColorBrewer)
rbPal <- colorRampPalette(c(brewer.pal(9, "OrRd"),"black"))
col <- rbPal(10)[as.numeric(cut(ard,
                                breaks = quantile(ard,seq(0,1,by=.1))))]
col2 <- rbPal(10)[as.numeric(cut(pprn,
                                breaks = quantile(pprn,seq(0,1,by=.1))))]

col3<-ifelse(finalStatus=="ARD","black","red")
col4<-ifelse(ard>pprn,"black","red")
#col4<-ifelse((ard>pprn & finalStatus=="ARD") | (pprn>ard & finalStatus=="PPRM") ,"black","red")
pts=cbind(x=c(-119,-118,-118,-119),y=c(41,41,39,39)) # location for legend

pdf('test.pdf', width=12, height=10)
par(mar = c(0,0,0,0),mfrow=c(2,2))

map("state","california")
plot(caFull,add=T,col=col)
text(-117,40,"Annual Renewal Demand",cex=1.2)

map("state","california")
plot(caFull,add=T,col=col2)
text(-117,40,"Pro-Rata Need",cex=1.2)

map("state","california")
plot(caFull,add=T,col=col3)
legend.gradient(pts,c("black","red"),c("recieved ARD","received PPRN"),cex=1.2,title="")

map("state","california")
plot(caFull,add=T,col=col3)
legend.gradient(pts,c("black","red"),c("ARD>PPRN","ARD<=PPRN"),cex=1.2,title="")
dev.off()
#legend.gradient(pts,rbPal(10),c(min(ard),max(ard)),cex=.75)


# text(-116,40,median(ard),cex=.75)
# 
# points(-119,41,col="blue",cex=2,pch=17)
# points(-116,41,col="blue",cex=2,pch=17)
# points(-116,39,col="blue",cex=2,pch=17)
# points(-119,39,col="blue",cex=2,pch=17)

#options(scipen=0)

## weather stations in california

weatherStations<-read.csv("weatherStations.csv") ## Sara has a copy of this from another project
## cleaned up from a gross file on Wunderground
names(weatherStations)

library(sp)
library(maps)
library(maptools)

# The single argument to this function, pointsDF, is a data.frame in which:
#   - column 1 contains the longitude in degrees (negative in the US)
#   - column 2 contains the latitude in degrees
#http://stackoverflow.com/questions/8751497/latitude-longitude-coordinates-to-state-code-in-r/8751965#8751965
latlong2state <- function(pointsDF) {
  # Prepare SpatialPolygons object with one SpatialPolygon
  # per state (plus DC, minus HI & AK)
  states <- map('state', fill=TRUE, col="transparent", plot=FALSE)
  IDs <- sapply(strsplit(states$names, ":"), function(x) x[1])
  states_sp <- map2SpatialPolygons(states, IDs=IDs),
                                   proj4string=CRS("+proj=longlat +datum=wgs84"))
  
  # Convert pointsDF to a SpatialPoints object 
  pointsSP <- SpatialPoints(pointsDF, 
                            proj4string=CRS("+proj=longlat +datum=wgs84"))
  
  # Use 'over' to get _indices_ of the Polygons object containing each point 
  indices <- over(pointsSP, states_sp)
  
  # Return the state names of the Polygons object containing each point
  stateNames <- sapply(states_sp@polygons, function(x) x@ID)
  stateNames[indices]
}

missing=unique(union(which(is.na(weatherStations$longitude)),which(is.na(weatherStations$latitude))))
weatherStations=weatherStations[-missing,] ## remove stations with missing values of lat long
weatherStations$state=latlong2state(weatherStations[,c("longitude","latitude")])


caWeatherStations=subset(weatherStations,state=="california")
nrow(caWeatherStations)
map("state","california")
points(caWeatherStations$longitude, caWeatherStations$latitude,pch=19)

require(weatherData)

## check for data availability
isData=unlist(lapply(1:nrow(caWeatherStations),function(x){checkSummarizedDataAvailability(caWeatherStations$code[x],"2014-01-01","2014-12-31")}))
sum(isData)/length(isData)

## go grab data of interest that is available, takes a few minutes
ptm <- proc.time()
getData=lapply((1:nrow(caWeatherStations))[which(isData==1)],function(x){getSummarizedWeather(caWeatherStations$code[x],"2014-01-01","2014-12-31",opt_custom_columns=T,custom_columns=c(2,3,4,8,9,10,17,18,20))})
proc.time() - ptm

save(getData,file="caMonthlyWeather2014.RData")

head(getData[[1]])

## find closest weather station to a location
## do we take the middle of each CoC, an average?
require(fields)

## UNCOMMENT WHEN HAVE DATA
# dataOfInterest=
# 
# distances=rdist.earth(
#   matrix(c(dataOfInterest$lonitude,dataOfInterest$lat), ncol=2),
#   matrix(cbind(as.numeric(weatherStations$longitude), 
#                as.numeric(weatherStations$latitude)), ncol=2),
#   miles=FALSE, R=6371) # not Euclidean distance, Great Circle Distance
# 
# closest=apply(distances,1,which.min)

## ISTI data set

setwd("~/Desktop/ds421_brainstorm/RegionalHomogenization/data")
load("networks.rda")
load("stationInfo.rda")
head(stationInfo)
head(networks[[1]])


download.file("ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank  
              /monthly/stage3/recommended/results/recommended
              -netcdf_format.monthly.stage3.v1.0.1.20150605
              .tar.gz","ISTImonthly.tar.gz")

## un-compress- this process will take at least 5 minutes.


untar("ISTImonthly.tar.gz")


## When this is done, there will be two new folders in your workspace: 
## netcdf_merged (contains data of interest) and netcdf_withheld (not used)

##I have a cleaned list of station information from working with this data before.
##Original: ftp://ftp.ncdc.noaa.gov/pub/data/globaldatabank/monthly/stage3/recommended/results/INVENTORY_monthly_merged_stage3

stationInfo<-read.csv("stationInfo.csv")

## remove all outside US
## can translate lat long into states as before

stationID= ## list of station IDs of interest

## UNCOMMENT WHEN stationID is filled  
# network <- vector("list", length(stationID))
# for(i in 1:length(neighborID)){
#   data<-ncdf::open.ncdf(paste("netcdf_merged/",stationID[i],".nc",sep=""))
#   time<-ncdf::get.var.ncdf(data,"time")
#   tt = as.Date("1600-01-01") + time
#   year=as.numeric(format(tt, "%Y"))
#   month=as.numeric(format(tt, "%m"))
#   avgTemp<-ncdf::get.var.ncdf(data,"surface_average_temperature")
#   maxTemp<-ncdf::get.var.ncdf(data,"surface_maximum_temperature")
#   minTemp<-ncdf::get.var.ncdf(data,"surface_minimum_temperature")
#   ncdf::close.ncdf(data)
#   dataDF=as.data.frame(cbind(year,month,avgTemp,maxTemp,minTemp))
#   network[[i]]=dataDF
#   
# }
# names(network)=stationID
